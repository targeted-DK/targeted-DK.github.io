---
layout: post
title: "Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval"
date: 2024-10-16
categories: papers
---

# Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval
 
## Overview

Composed Image Retrieval (CIR)은 query image와 텍스트를 결합하여 target image를 생성하기 위해 만들어졌다. 이미 존재하는 방식은 query image, text specification, target image으로 구성되어 있다.
하지만 이 세 개로 구성된 묶음을 레이블링하는건 매우 비싸며 CIR의 범용성을 저해한다. 이 논문에서는 Zero-Shot Composed Image Retrieval(ZS-CIR)을 제시하며, 이 CIR 모델은
훈련에 필요한 레이블링 없이 진행된다. 이 모델에서는 Pic2Word라는 새로운 방식을 제시하며 이는 'weakly' 레이블된 이미지-캡션 쌍과 레이블이 없는 이미지 데이터 셋을 훈련에 사용한다.
이미 존재하는 CIR모델들 과는 달리, 이 모델은 weakly 레이블된 또는 레이블이 없는 데이터 셋으로 훈련하였으며 이는 여러 방면(attribute editing, object composition, domain conversion)에서 강한 generalization을 보여준다.


## Introduction
CIR는 이미지와 텍스트로 구성된 query로 이미지를 생성한다. 이미지 기반 생성 시스템과는 달리 CIR는 더 높은 정확성을 보여주는데 이는 사용자의 의도가 들어간 텍스트가 포함되어 있기 때문이다.
이미지-텍스트 모델의 증가로 인해 CIR는 온라인 판매나 인터넷 검색과 같은 실생활 부문에서 많은 관심을 받아왔다.

여러 CIR 문제들을 해결하기 위해 (패션 이미지 검색, 컨텐츠 생성 등) 가장 중요한 부분은 이미지와 텍스트에서 정보를 배우는(learning)것이다. 하지만 여기에 두가지 문제점이 있는데, 첫째로는 많은 레이블된 데이터가 필요하다는 것이고, 이는 세 개의 데이터로 이뤄진 쌍으로 이루어져있다.
데이터 수집은 관련된 reference image와 target image를 query-output pair로 CIR 시스템에 넣어야하며 reference를 target으로 바꾸는 description text가 필요하다. 두번째로는 이렇게 생성된 레이블 데이터는 특정 상황에만 사용되어
일반화하기에는 매우 어려울 수 있다.

이러한 문제점을 해결하기 위해서 *zero-shot composed image retrieval (ZS-CIR)* 를 제시한다. 이 모델은 image-caption페어와 레이블 없는 이미지를 활용하여 매우 싼 비용으로 모델을 생성 할 수 있다.
![](/images/Pic2Word/1.png)

'weakly' labeled 데이터 셋과 레이블이 없는 데이터 셋을 사용하기 위해서, 첫 번째 단계에서는 이미지-캡션 데이터셋에 CLIP을 사용하여 이미지와 캡션의 유사도를 최대화 한다. 바로 supervised CIR training보다는
CLIP 모델 인코더의 언어적 능력을 활용하기 위해 *picture to a word token* 개념을 활용하여 사진을 단어로 매핑하는 것이다. 이는 CLIP 비전 인코더의 이미지 임베딩을 언어 인코더의 토큰 임베딩으로 매핑하는 것이다.
이 매핑 네트워크는 contrastive loss로 훈련되었고 이는 레이블이 없는 이미지로 가능하다. 이를 **Pic2Word**라고 부른다.

이 논문에서는 Pic2Word 방식이 여러 CIR task에 적합하며, domain conversion, object composition, scene manipulation. fashion tribute manipulation등을 포함한다. 
- ZS_CIR : triplet 레이블 데이터셋 필요 없이 ZS-CIR를 구현
- Pic2Wrod : 이미지-캡션 쌍과 레이블 없는 데이터셋으로 훈련 가능한 모델 제시. 이는 입력 이미지를 언어 토큰으로 변환하여 image-text query 생성.
- Pic2Word는 ZS-CIR의 능력을 10-100프로 증가시킴.

![](/images/Pic2Word/2.png)

## Related Work

### Composed Image Retrieval
CIR는 이미지를 생성하기 위해 레퍼런스 이미지와 텍스트 쌍으로 구성되어 있다. 현재 CIR모델들은 후기 융합 과정에서 여러 시각과 언어 특징등을 여러 개의 다른 인코더로 추출하여 합친다. 기훈련된 CLIP 모델이 가장 좋은 성능을 보인다. 대부분 모델들은 기훈련된 언어 모델의 조합 능력을 트렌스포머 모델을 조율하여 CIR 데이터 셋에 사용하지만, 이 논문에서는 CIR 데이터 셋 없이도 여러 시나리오에서 사용될 수 있도록 하였다.

### Vision-language foundation models
CLIP과 ALIGN과 같은 비전언어모델은 수백만장의 이미지-캡션 쌍을 이미지와 언어 인코더 쌍에 훈련시키는 것이다. 클립 이후 여러 비전 언어 foundation 모델들은 더 많은 데이터와 새로운 모델 구조, 새로운 목적을 제시하였다. 이것과 비슷하게 CIR관련 task를 CIR 데이터셋 없이 적용하려고 한다. 이는 비전언어 모델에 contrastive learning을 대입하여 zero-shot 방식으로 CIR을 작동시키는 것이다.

### Representing image as one wrod
비전 언어 모델을 pre-training 할 때 이미지의 부분들을 토큰으로 표현하기 위한 여러 방식지 제안되었다. 일반적인 방법으로는 1) 기훈련된 object detector 통한 이미지에 있는 object 감지, 2) 특정 부분과 문장을 텍스트 인코더로 투입, 3) 여러 멀티 모달의 목표들을 최적화하여 성능이 높은 비전 언어 모델 습득으로 이어진다. 이런 접근 방식들은 훈련 단계에 성능이 높은 object detector를 요구하지만 우리는 훈련된 contrastive vision-language 모델을 제시한다. 
이미지 셋을 'concept' word token으로 변환시키기 위해 이미지와 class-wise 캡션 및 요약이 필요하다. 하지만 여기서는 composed image retrieval에 집중하여, 우리가 제시하는 Pic2Word에서는 조율된 단어 토큰이나 많은 annotation을 필요로 하지 않는다. 

## Method

ZS-CIR 기반으로한 Pic2Word는 언어 인코더와 비전 인코더로 이루어져 있으며, output 임베딩은 각 인코더의 modality를 반영한 출력물이다. 기본적으로 CLIP의 알고리즘을 사용한다.

![](/images/Pic2Word/3.png)
![](/images/Pic2Word/4.png)

비전 임베딩을 의사-언어 토큰으로 매핑하며, contrastive loss를 사용한다. 여기에는 frozen CLIP 언어 인코더가 사용된다. CLIP에서는 언어 인베딩 u와 이미지 임베딩 v가 나란히 최적화되어 사용된다. 의사-토큰은 이미지의 표현을 충분히 설명한다고 추측하며, 이는 generic prompt와 의사 토큰 임베딩이 관련된 이미지 임베딩과 유사하다면 이미지의 표현을 충분히 반영한다고 추측한다. 이 의견을 토대로 이 논문에서는 mapping network를 훈련하는데, 이는 의사 언어 토큰을 출력하도록 되어있으며 고정된 언어 인코더의 언어 임베딩이 이번과 언어 임베딩간의 contrastive loss를 최소화 할 수 있다. 

결론적으로, 매핑 네트워크는 언어 임베딩 공간에서 visual representation을 재생산 할 수 있도록 훈련되었다. 각 이미지를 한 단어 토큰으로 표현하면서도, 이미지의 정보를 충분히 표현하기 위해 여러 단어 토큰이 사용될 수 있다. 한 단어 토큰만으로도 충분히 이미지를 설명할 수 있다. 다만 여러 토큰이 사용될 경우 매우 디테일 한 이미지를 표현 할 수 있다고 추측한다. 다른 supervised CIR 방법과는 달리 Pic2Word는 언어 토큰 공간의 초기 단계에서 시각 및 언어 특징을 결합시킨다. 이는 훈련된 언어 모델을 여러 concept에 대입할 수 있도록 작용한디. 

### Inference

Inference에서는 이미지와 텍스트 query를 결합하여 이미지 후보군과 비교하는 것이다. 핵심은 의사 토큰을 pre-defined prompts에 진짜 단어 토큰인 것처럼 추가하는 것이다. 여기서의 결과물은 텍스트 인코더로 임베딩되며 이를 후보 이미지군의 시각적 특징과 비교하는 것이다. 최종 성능에는 프롬프트 구조가 매우 큰 영향을 미친다. 
(a) Domain Conversion :  [Domain] of [*]
(b) Object/Scene Composition : a photo of [obj]
(c) Sentence specification : a photo of [text]

## Analysis
- 토큰이 이미지의 주요 특징을 자 ㄹ표현한다.
- 각 데이터셋에 사용되는 최적 가중치가 다를 수 있으며 이 가중치에 따라 성능이 매우 달라진다.
- 적은 훈련 샘플로 supervised baseline보다 더 나은 성능을 보여준다.
- 자연 관련 이미지들에서 retrieval이 실패하는 경우가 있다. 
