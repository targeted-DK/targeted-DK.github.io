---
layout: post
title: "Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval"
date: 2024-10-16
categories: papers
---

# Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval
 
## Overview

Composed Image Retrieval (CIR)은 query image와 텍스트를 결합하여 target image를 생성하기 위해 만들어졌다. 이미 존재하는 방식은 query image, text specification, target image으로 구성되어 있다.
하지만 이 세 개로 구성된 묶음을 레이블링하는건 매우 비싸며 CIR의 범용성을 저해한다. 이 논문에서는 Zero-Shot Composed Image Retrieval(ZS-CIR)을 제시하며, 이 CIR 모델은
훈련에 필요한 레이블링 없이 진행된다. 이 모델에서는 Pic2Word라는 새로운 방식을 제시하며 이는 'weakly' 레이블된 이미지-캡션 쌍과 레이블이 없는 이미지 데이터 셋을 훈련에 사용한다.
이미 존재하는 CIR모델들 과는 달리, 이 모델은 weakly 레이블된 또는 레이블이 없는 데이터 셋으로 훈련하였으며 이는 여러 방면(attribute editing, object composition, domain conversion)에서 강한 generalization을 보여준다.


## Introduction
CIR는 이미지와 텍스트로 구성된 query로 이미지를 생성한다. 이미지 기반 생성 시스템과는 달리 CIR는 더 높은 정확성을 보여주는데 이는 사용자의 의도가 들어간 텍스트가 포함되어 있기 때문이다.
이미지-텍스트 모델의 증가로 인해 CIR는 온라인 판매나 인터넷 검색과 같은 실생활 부문에서 많은 관심을 받아왔다.

여러 CIR 문제들을 해결하기 위해 (패션 이미지 검색, 컨텐츠 생성 등) 가장 중요한 부분은 이미지와 텍스트에서 정보를 배우는(learning)것이다. 하지만 여기에 두가지 문제점이 있는데, 첫째로는 많은 레이블된 데이터가 필요하다는 것이고, 이는 세 개의 데이터로 이뤄진 쌍으로 이루어져있다.
데이터 수집은 관련된 reference image와 target image를 query-output pair로 CIR 시스템에 넣어야하며 reference를 target으로 바꾸는 description text가 필요하다. 두번째로는 이렇게 생성된 레이블 데이터는 특정 상황에만 사용되어
일반화하기에는 매우 어려울 수 있다.

이러한 문제점을 해결하기 위해서 *zero-shot composed image retrieval (ZS-CIR)* 를 제시한다. 이 모델은 image-caption페어와 레이블 없는 이미지를 활용하여 매우 싼 비용으로 모델을 생성 할 수 있다.

'weakly' labeled 데이터 셋과 레이블이 없는 데이터 셋을 사용하기 위해서, 첫 번째 단계에서는 이미지-캡션 데이터셋에 CLIP을 사용하여 이미지와 캡션의 유사도를 최대화 한다. 바로 supervised CIR training보다는
CLIP 모델 인코더의 언어적 능력을 활용하기 위해 *picture to a word token* 개념을 활용하여 사진을 단어로 매핑하는 것이다. 이는 CLIP 비전 인코더의 이미지 임베딩을 언어 인코더의 토큰 임베딩으로 매핑하는 것이다.
이 매핑 네트워크는 contrastive loss로 훈련되었고 이는 레이블이 없는 이미지로 가능하다. 이를 **Pic2Word**라고 부른다,

![](/images/Plenoxels/4.png)

